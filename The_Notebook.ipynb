{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DYNOSuprovo/MachineLearning/blob/main/The_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEcr4jdzTP7t"
      },
      "source": [
        "# **Welcome To the Notebook**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1eOogPqix4i"
      },
      "source": [
        "### **Task 1 - Loading our data**\n",
        "\n",
        "Installing the pyspark using pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYfUECtFisDa",
        "outputId": "cd096bb5-f97a-4244-c9cb-ab2eb94e4af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m316.9/317.0 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr4LlOxmvUmW"
      },
      "source": [
        "Importing Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiE8RPAHvWHD"
      },
      "outputs": [],
      "source": [
        "# importing spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# data visualization modules\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "# pandas module\n",
        "import pandas as pd\n",
        "\n",
        "# pyspark SQL functions\n",
        "from pyspark.sql.functions import col, when, count, udf\n",
        "\n",
        "# pyspark data preprocessing modules\n",
        "from pyspark.ml.feature import Imputer, StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\n",
        "\n",
        "# pyspark data modeling and model evaluation modules\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-_0aGYvj6sI"
      },
      "source": [
        "Building our Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQgz_0Zgiilu"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"Customer_Churn_Prediction\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1aneLUYvh71"
      },
      "source": [
        "Loading our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3Y1melCvqN5"
      },
      "outputs": [],
      "source": [
        "data=spark.read.format('csv').option('header',  True).option('inferSchema','true').option('header',True).load(\"dataset.csv\")\n",
        "data.show(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeY_Znoi4WQA"
      },
      "source": [
        "Print the data schema to check out the data types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yopaRXr4Ufr"
      },
      "outputs": [],
      "source": [
        "data.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od0SPukD3kPI"
      },
      "source": [
        "Get the data dimension"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lEn4GaGNpa54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW0B267Wv0tm"
      },
      "outputs": [],
      "source": [
        "data.count()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZzDBEKT2H9P"
      },
      "source": [
        "### **Task 2 - Exploratory Data Analysis**\n",
        "- Distribution Analysis\n",
        "- Correlation Analysis\n",
        "- Univariate Analysis\n",
        "- Finding Missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUWEEw9VTVor"
      },
      "source": [
        "Let's define some lists to store different column names with different data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4cJoyG-Ubel"
      },
      "outputs": [],
      "source": [
        "numerical_columns = [name for name,typ in data.dtypes if typ==\"double\" or typ =='int']\n",
        "categorical_columns = [name for name,typ in data.dtypes if typ==\"string\" ]\n",
        "data.select(numerical_columns).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYoYREY75vfo"
      },
      "source": [
        "Let's get all the numerical features and store them into a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWYeP2xp45Ot"
      },
      "outputs": [],
      "source": [
        "df=data.select(numerical_columns).toPandas()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZgNnbgnNtTe"
      },
      "source": [
        "Let's create histograms to analyse the distribution of our numerical columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gizkvQ3oNsuo"
      },
      "outputs": [],
      "source": [
        "fig=plt.figure(figsize=(15,10))\n",
        "ax=fig.gca()\n",
        "df.hist(ax=ax,bins=20)\n",
        "df.tenure.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qjCcR72ru2_"
      },
      "source": [
        "Let's generate the correlation matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldNx2GuVrvAR"
      },
      "outputs": [],
      "source": [
        "df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbBRqQQJhmqg"
      },
      "source": [
        "Let's check the unique value count per each categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgXj3fjJb0th"
      },
      "outputs": [],
      "source": [
        "for column in categorical_columns:\n",
        "  data.groupBy(column).count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgmiBcimTw5z"
      },
      "source": [
        "Let's find number of null values in all of our dataframe columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vlBo7LWT1kB"
      },
      "outputs": [],
      "source": [
        "for column in data.columns:\n",
        "  data.select(count(when(col(column).isNull(),column)).alias(column)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcTLE0ZdS-so"
      },
      "source": [
        "### **Task 3 - Data Preprocessing**\n",
        "- Handling the missing values\n",
        "- Removing the outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDKvkUiwzt_H"
      },
      "source": [
        "**Handling the missing values** <br>\n",
        "Let's create a list of column names with missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABZfAKxRS3CN"
      },
      "outputs": [],
      "source": [
        "columns_with_missing_values = [\"TotalCharges\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny4cfwKYzzJN"
      },
      "source": [
        "Creating our Imputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqBhbzyNzyFs"
      },
      "outputs": [],
      "source": [
        "imputer=Imputer(inputCols=columns_with_missing_values,outputCols=columns_with_missing_values).setStrategy(\"mean\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XgghJUpz6wv"
      },
      "source": [
        "Use Imputer to fill the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmMfG-Gqz4m4"
      },
      "outputs": [],
      "source": [
        "imputer=imputer.fit(data)\n",
        "data=imputer.transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fKFfw2G0A4j"
      },
      "source": [
        "Let's check the missing value counts again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LscLsydL0Dg_"
      },
      "outputs": [],
      "source": [
        "data.select(count(when(col(\"TotalCharges\").isNull(),\"TotalCharges\")).alias(\"TotalCharges\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9JJoI4_1Ao5"
      },
      "source": [
        "**Removing the outliers** <br>\n",
        "Let's find the customer with the tenure higher than 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuUgYzfM1ICN"
      },
      "outputs": [],
      "source": [
        "data.select(\"*\").where(data.tenure>100).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcaebVkh2CoU"
      },
      "source": [
        "Let's drop the outlier row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANT7iLlB2Et5"
      },
      "outputs": [],
      "source": [
        "print(\"Before removing the outliers\",data.count())\n",
        "data=data.filter(data.tenure<100)\n",
        "print(\"After removing the outliers\",data.count())\n",
        "data.select(\"*\").where(data.tenure>100).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQMD4VIuPDen"
      },
      "source": [
        "### **Task 4 - Feature Preparation**\n",
        "- Numerical Features\n",
        "    - Vector Assembling\n",
        "    - Numerical Scaling\n",
        "- Categorical Features\n",
        "    - String Indexing\n",
        "    - Vector Assembling\n",
        "\n",
        "- Combining the numerical and categorical feature vectors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Feature Preparation - Numerical Features** <br>\n",
        "\n",
        "`Vector Assembling --> Standard Scaling` <br>\n",
        "\n",
        "**Vector Assembling** <br>\n",
        "To apply our machine learning model we need to combine all of our numerical and categorical features into vectors. For now let's create a feature vector for our numerical columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_g7Hu3HPC9Q"
      },
      "outputs": [],
      "source": [
        "numerical_vector_assembler=VectorAssembler(inputCols=numerical_columns, outputCol=\"numerical_features_vector\")\n",
        "data=numerical_vector_assembler.transform(data)\n",
        "data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U39MK-kfYnpc"
      },
      "source": [
        "**Numerical Scaling** <br>\n",
        "Let's standardize all of our numerical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqvD874WSQEs"
      },
      "outputs": [],
      "source": [
        "scaler=StandardScaler(inputCol=\"numerical_features_vector\", outputCol=\"numerical_features_scaled\", withStd=True, withMean=True)\n",
        "data=scaler.fit(data).transform(data)\n",
        "data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfbVsiv6Z1SM"
      },
      "source": [
        "**Feature Preperation - Categorical Features** <br>\n",
        "\n",
        "`String Indexing --> Vector Assembling` <br>\n",
        "\n",
        "**String Indexing** <br>\n",
        "We need to convert all the string columns to numeric columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7xmubmpZ1zo"
      },
      "outputs": [],
      "source": [
        "categorical_columns_indexed=[name+\"_Indexed\" for name in categorical_columns]\n",
        "indexer=StringIndexer(inputCols=categorical_columns, outputCols=categorical_columns_indexed)\n",
        "data=indexer.fit(data).transform(data)\n",
        "data.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "566hiGhl_1m0"
      },
      "source": [
        "Let's combine all of our categorifal features in to one feature vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh0qcHDb6q8t"
      },
      "outputs": [],
      "source": [
        "if \"customerID_Indexed\" in categorical_columns_indexed:\n",
        "    categorical_columns_indexed.remove(\"customerID_Indexed\")\n",
        "if \"Churn_Indexed\" in categorical_columns_indexed:\n",
        "    categorical_columns_indexed.remove(\"Churn_Indexed\")\n",
        "\n",
        "categorical_vector_assembler=VectorAssembler(inputCols=categorical_columns_indexed,outputCol=\"categorical_features_vector\")\n",
        "data=categorical_vector_assembler.transform(data)\n",
        "data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G0bVH-vALJt"
      },
      "source": [
        "Now let's combine categorical and numerical feature vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrsFk-CZZgrV"
      },
      "outputs": [],
      "source": [
        "final_vector_assembler=VectorAssembler(inputCols=[\"categorical_features_vector\",\"numerical_features_scaled\"],outputCol=\"final_feature_vector\")\n",
        "data=final_vector_assembler.transform(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_og11F0MdrYE"
      },
      "source": [
        "### **Task 5 - Model Training**\n",
        "- Train and Test data splitting\n",
        "- Creating our model\n",
        "- Training our model\n",
        "- Make initial predictions using our model\n",
        "\n",
        "In this task, we are going to start training our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VFwkblddkdV"
      },
      "outputs": [],
      "source": [
        "train,test=data.randomSplit([0.7,0.3],seed=100)\n",
        "train.count()\n",
        "test.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiAJrl8oEKIk"
      },
      "source": [
        "Now let's create and train our desicion tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bObhrM_4nKCf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "dt=DecisionTreeClassifier(featuresCol=\"final_feature_vector\",labelCol=\"Churn_Indexed\",maxDepth=3)\n",
        "dtModel=dt.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy_-7W2LEO9j"
      },
      "source": [
        "Let's make predictions on our test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqU9-y6dEOqq"
      },
      "outputs": [],
      "source": [
        "predictions_test=dtModel.transform(test)\n",
        "predictions_test.select([\"Churn\",\"prediction\"]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZXX1rqz8hoT"
      },
      "source": [
        "### **Task 6 - Model Evaluation**\n",
        "- Calculating area under the ROC curve for the `test` set\n",
        "- Calculating area under the ROC curve for the `training` set\n",
        "- Hyper parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8Ldw-rno18o"
      },
      "outputs": [],
      "source": [
        "evaluator=BinaryClassificationEvaluator(labelCol=\"Churn_Indexed\")\n",
        "auc_test=evaluator.evaluate(predictions_test,{evaluator.metricName:\"areaUnderROC\"})\n",
        "auc_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXpG_p4BGEhq"
      },
      "source": [
        "Let's get the AUC for our `training` set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVkrmoC8GErZ"
      },
      "outputs": [],
      "source": [
        "predictions_test=dtModel.transform(train)\n",
        "auc_train=evaluator.evaluate(predictions_test,{evaluator.metricName:\"areaUnderROC\"}) # Use predictions_test since that's what you just calculated\n",
        "auc_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH4ZH3yFHbvS"
      },
      "source": [
        "**Hyper parameter tuning**\n",
        "\n",
        "Let's find the best `maxDepth` parameter for our DT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwkzV8xLHNlE"
      },
      "outputs": [],
      "source": [
        "def evaluate_dt(mode_params):\n",
        "      test_accuracies = []\n",
        "      train_accuracies = []\n",
        "\n",
        "      for maxD in mode_params:\n",
        "        # train the model based on the maxD\n",
        "        decision_tree = DecisionTreeClassifier(featuresCol = 'final_feature_vector', labelCol = 'Churn_Indexed', maxDepth = maxD)\n",
        "        dtModel = decision_tree.fit(train)\n",
        "\n",
        "        # calculating test error\n",
        "        predictions_test = dtModel.transform(test)\n",
        "        evaluator = BinaryClassificationEvaluator(labelCol=\"Churn_Indexed\")\n",
        "        auc_test = evaluator.evaluate(predictions_test, {evaluator.metricName: \"areaUnderROC\"})\n",
        "        # recording the accuracy\n",
        "        test_accuracies.append(auc_test)\n",
        "\n",
        "        # calculating training error\n",
        "        predictions_training = dtModel.transform(train)\n",
        "        evaluator = BinaryClassificationEvaluator(labelCol=\"Churn_Indexed\")\n",
        "        auc_training = evaluator.evaluate(predictions_training, {evaluator.metricName: \"areaUnderROC\"})\n",
        "        train_accuracies.append(auc_training)\n",
        "\n",
        "      return(test_accuracies, train_accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37QrYjWvcE4Z"
      },
      "source": [
        "Let's define `params` list to evaluate our model iteratively with differe maxDepth parameter.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rVz17-jIoVC"
      },
      "outputs": [],
      "source": [
        "maxDepths=[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
        "test_accs,train_accs=evaluate_dt(maxDepths)\n",
        "print(train_accs)\n",
        "print(test_accs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uBlqQ2-cWCm"
      },
      "source": [
        "Let's visualize our results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqFqOfKvJATk"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame()\n",
        "df[\"maxDepth\"]=maxDepths\n",
        "df[\"test_Accs\"]=test_accs\n",
        "df[\"train_Accs\"]=train_accs\n",
        "px.line(df,x=\"maxDepth\",y=[\"train_Accs\",\"test_Accs\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koCUPBFUKDIZ"
      },
      "source": [
        "### **7 - Model Deployment**\n",
        "- Giving Recommendations using our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xomU5qnMUdWJ"
      },
      "source": [
        "\n",
        "\n",
        "We were asked to recommend a solution to reduce the customer churn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FIFIqdLsAvq"
      },
      "outputs": [],
      "source": [
        "feature_importance=dtModel.featureImportances\n",
        "scores=[score for i,score in enumerate(feature_importance)]\n",
        "df=pd.DataFrame(scores,columns=[\"score\"],index=categorical_columns_indexed+numerical_columns)\n",
        "px.bar(df,y=\"score\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.groupby([\"Contract_Indexed\",\"Churn\"]).count().toPandas() # Use () for groupby\n",
        "px.bar(df,x=\"Contract_Indexed\",y=\"count\",color=\"Churn\")"
      ],
      "metadata": {
        "id": "G6OY2SV6XTM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_FrIiJwWuwg"
      },
      "source": [
        "Let's create a bar chart to visualize the customer churn per contract type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4eObw2aK7Qp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhaY0_iA6pp1"
      },
      "source": [
        "The bar chart displays the number of churned customers based on their contract type. It is evident that customers with a \"Month-to-month\" contract have a higher churn rate compared to those with \"One year\" or \"Two year\" contracts. As a recommendation, the telecommunication company could consider offering incentives or discounts to encourage customers with month-to-month contracts to switch to longer-term contracts."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "koCUPBFUKDIZ"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}